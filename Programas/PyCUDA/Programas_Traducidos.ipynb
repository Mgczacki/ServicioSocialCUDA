{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traduciendo CUDA-C a PyCUDA\n",
    "\n",
    "Dado que los kernels de PyCUDA siguen siendo código CUDA-C, es posible traducir los programas que hemos realizado previamente a PyCUDA de forma sencilla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vector_add_parallel.cu\n",
    "\n",
    "``` \n",
    "#define N 400000000\n",
    "#include <iostream>\n",
    "#include <math.h>\n",
    "\n",
    "__global__ void vector_add(float *out, float *a, float *b, int n) {\n",
    "    int indice = threadIdx.x;//Indice del thread que ejecuta el kernel\n",
    "    int paso = blockDim.x;//El numero de threads por bloque\n",
    "    for(int i = indice; i < n; i+=paso){\n",
    "        out[i] = a[i] + b[i];\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(){\n",
    "    float *a, *b, *out; //Apuntadores a memoria del anfitrión\n",
    "    float *cuda_a, *cuda_b, *cuda_out; //Apuntadores a memoria del GPU\n",
    "    //Generamos los arreglos en memoria del GPU\n",
    "    cudaMalloc((void**)&cuda_a, sizeof(float) * N);\n",
    "    cudaMalloc((void**)&cuda_b, sizeof(float) * N);\n",
    "    cudaMalloc((void**)&cuda_out, sizeof(float) * N);\n",
    "    //Generamos los arreglos en memoria del anfitrión\n",
    "    a   = (float*)malloc(sizeof(float) * N);\n",
    "    b   = (float*)malloc(sizeof(float) * N);\n",
    "    out = (float*)malloc(sizeof(float) * N);\n",
    "    // Inicializamos a y b\n",
    "    for(int i = 0; i < N; i++){\n",
    "        a[i] = 1.0f; b[i] = 2.0f;\n",
    "    }\n",
    "    //Copiamos los vectores a y b al GPU.\n",
    "    cudaMemcpy(cuda_a, a, sizeof(float) * N, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(cuda_b, b, sizeof(float) * N, cudaMemcpyHostToDevice);\n",
    "    // Llamamos al kernel de CUDA (1 bloque, 256 threads por bloque).\n",
    "    vector_add<<<1,256>>>(cuda_out, cuda_a, cuda_b, N);\n",
    "    //Copiamos el vector de salida del GPU al anfitrión.\n",
    "    cudaMemcpy(out, cuda_out, sizeof(float) * N, cudaMemcpyDeviceToHost);\n",
    "    float maxError = 0.0f;\n",
    "    for (int i = 0; i < N; i++)\n",
    "        maxError = fmax(maxError, fabs(out[i]-3.0f));\n",
    "    std::cout << \"Max error: \" << maxError << std::endl;\n",
    "    cudaFree(cuda_a);\n",
    "    cudaFree(cuda_b);\n",
    "    cudaFree(cuda_out);\n",
    "    free(a);\n",
    "    free(b);\n",
    "    free(out);\n",
    "    //Sugiero ver el comando: nvprof ./vector_add_parallel\n",
    "}\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora lo reescribiremos a PyCUDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo: 1825.8001708984375 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pycuda.autoinit\n",
    "import pycuda.driver as drv\n",
    "import numpy\n",
    "\n",
    "from pycuda.compiler import SourceModule\n",
    "\n",
    "mod = SourceModule(\"\"\"\n",
    "__global__ void vector_add(float *out, float *a, float *b, int n) {\n",
    "    int indice = blockIdx.x * blockDim.x + threadIdx.x;//Indice del thread que ejecuta el kernel\n",
    "    int paso = blockDim.x * gridDim.x;//El numero de threads por bloque\n",
    "    for(int i = indice; i < n; i+=paso){\n",
    "        out[i] = a[i] + b[i];\n",
    "    }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "vector_add = mod.get_function(\"vector_add\")\n",
    "a = numpy.ones(400000000).astype(numpy.float32)\n",
    "b = 2*numpy.ones(400000000).astype(numpy.float32)\n",
    "\n",
    "resultado_esperado = a+b\n",
    "\n",
    "dest = numpy.zeros_like(a)\n",
    "\n",
    "start = drv.Event()\n",
    "stop = drv.Event()\n",
    "start.record()\n",
    "\n",
    "vector_add(\n",
    "        drv.Out(dest), drv.In(a), drv.In(b), numpy.int32(400000000),\n",
    "        block=(256,1,1), grid=(100,1))\n",
    "\n",
    "stop.record()\n",
    "stop.synchronize()\n",
    "time = start.time_till(stop)\n",
    "print(\"Tiempo: \"+str(time) + \" ms\")\n",
    "\n",
    "a_confirmacion = (resultado_esperado-dest).sum()\n",
    "a_confirmacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo de utilización de Streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Código original: \n",
    "``` \n",
    "#define N 100000000\n",
    "#include <iostream>\n",
    "#include <math.h>\n",
    "\n",
    "__global__ void vector_add(float *out, float *a, float *b, int n_max, int offset) {\n",
    "    int indice = offset + blockIdx.x * blockDim.x + threadIdx.x;//Indice del thread que ejecuta el kernel\n",
    "    int paso = blockDim.x * gridDim.x;//El numero de threads por bloque\n",
    "    for(int i = indice; i < n_max; i+=paso){\n",
    "        out[i] = a[i] + b[i];\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(){\n",
    "    float *a, *b, *out; //Apuntadores a memoria del anfitrión\n",
    "    float *cuda_a, *cuda_b, *cuda_out; //Apuntadores a memoria del GPU\n",
    "    //Generamos los arreglos en memoria del GPU\n",
    "    cudaMalloc((void**)&cuda_a, sizeof(float) * N);\n",
    "    cudaMalloc((void**)&cuda_b, sizeof(float) * N);\n",
    "    cudaMalloc((void**)&cuda_out, sizeof(float) * N);\n",
    "    //Generamos los arreglos en memoria del anfitrión\n",
    "    a   = (float*)malloc(sizeof(float) * N);\n",
    "    b   = (float*)malloc(sizeof(float) * N);\n",
    "    out = (float*)malloc(sizeof(float) * N);\n",
    "    // Inicializamos a y b\n",
    "    for(int i = 0; i < N; i++){\n",
    "        a[i] = 1.0f; b[i] = 2.0f;\n",
    "    }\n",
    "    \n",
    "    float ms;\n",
    "    cudaEvent_t startEvent, stopEvent, dummyEvent;\n",
    "    cudaEventCreate(&startEvent);\n",
    "    cudaEventCreate(&stopEvent);\n",
    "    cudaEventCreate(&dummyEvent);\n",
    "    //Versión con un solo stream.\n",
    "    cudaEventRecord(startEvent,0);\n",
    "    cudaMemcpy(cuda_a, a, sizeof(float) * N, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(cuda_b, b, sizeof(float) * N, cudaMemcpyHostToDevice);\n",
    "    vector_add<<<100, 256>>>(cuda_out, cuda_a, cuda_b, N, 0);\n",
    "    cudaMemcpy(out, cuda_out, sizeof(float) * N, cudaMemcpyDeviceToHost);\n",
    "    cudaEventRecord(stopEvent, 0);\n",
    "    cudaEventSynchronize(stopEvent);\n",
    "    cudaEventElapsedTime(&ms, startEvent, stopEvent);\n",
    "    printf(\"Tiempo para un solo stream (ms): %f\\n\", ms);\n",
    "    float maxError = 0.0f;\n",
    "    for (int i = 0; i < N; i++)\n",
    "        maxError = fmax(maxError, fabs(out[i]-3.0f));\n",
    "    std::cout << \"Max error: \" << maxError << std::endl;\n",
    "    //Versiones con múltiples streams - Iteraciones múltiples\n",
    "    //Haremos 10 streams distintos.\n",
    "    int n_streams = 10;\n",
    "    cudaStream_t stream[n_streams];\n",
    "    for (int i = 0; i < n_streams; i ++)\n",
    "    {\n",
    "        cudaStreamCreate(&stream[i]);\n",
    "    }\n",
    "    cudaEventRecord(startEvent,0);\n",
    "    int streamSize = N / n_streams;\n",
    "    int streamBytes = sizeof(float) * N / n_streams;\n",
    "    for (int i = 0; i < n_streams; i ++) \n",
    "    {\n",
    "        int offset = i * streamSize;\n",
    "        cudaMemcpyAsync(&cuda_a[offset], &a[offset], streamBytes, cudaMemcpyHostToDevice, stream[i]);\n",
    "        cudaMemcpyAsync(&cuda_b[offset], &b[offset], streamBytes, cudaMemcpyHostToDevice, stream[i]);\n",
    "    }\n",
    "    for (int i = 0; i < n_streams; ++i)\n",
    "    {\n",
    "        int offset = i * streamSize;\n",
    "        vector_add<<<100, 256, 0, stream[i]>>>(cuda_out, cuda_a, cuda_b, offset+streamSize, offset);\n",
    "    }\n",
    "    for (int i = 0; i < n_streams; i ++) \n",
    "    {\n",
    "        int offset = i * streamSize;\n",
    "        cudaMemcpyAsync(&out[offset], &cuda_out[offset], streamBytes, cudaMemcpyDeviceToHost, stream[i]);\n",
    "    }\n",
    "    cudaEventRecord(stopEvent, 0);\n",
    "    cudaEventSynchronize(stopEvent);\n",
    "    cudaEventElapsedTime(&ms, startEvent, stopEvent);\n",
    "    printf(\"Tiempo con múltiples streams - Separación de tareas(ms): %f\\n\", ms);\n",
    "    maxError = 0.0f;\n",
    "    for (int i = 0; i < N; i++)\n",
    "        maxError = fmax(maxError, fabs(out[i]-3.0f));\n",
    "    std::cout << \"Max error: \" << maxError << std::endl;\n",
    "    //Versión con múltiples streams - Una iteración por stream.\n",
    "    cudaEventRecord(startEvent,0);\n",
    "    for (int i = 0; i < n_streams; i ++) \n",
    "    {\n",
    "        int offset = i * streamSize;\n",
    "        cudaMemcpyAsync(&cuda_a[offset], &a[offset], streamBytes, cudaMemcpyHostToDevice, stream[i]);\n",
    "        cudaMemcpyAsync(&cuda_b[offset], &b[offset], streamBytes, cudaMemcpyHostToDevice, stream[i]);\n",
    "        vector_add<<<100, 256, 0, stream[i]>>>(cuda_out, cuda_a, cuda_b, offset+streamSize, offset);\n",
    "        cudaMemcpyAsync(&out[offset], &cuda_out[offset], streamBytes, cudaMemcpyDeviceToHost, stream[i]);\n",
    "    }\n",
    "    cudaEventRecord(stopEvent, 0);\n",
    "    cudaEventSynchronize(stopEvent);\n",
    "    cudaEventElapsedTime(&ms, startEvent, stopEvent);\n",
    "    printf(\"Tiempo con múltiples streams - Unificación de tareas (ms): %f\\n\", ms);\n",
    "    maxError = 0.0f;\n",
    "    for (int i = 0; i < N; i++)\n",
    "        maxError = fmax(maxError, fabs(out[i]-3.0f));\n",
    "    std::cout << \"Max error: \" << maxError << std::endl;\n",
    "    cudaFree(cuda_a);\n",
    "    cudaFree(cuda_b);\n",
    "    cudaFree(cuda_out);\n",
    "    free(a);\n",
    "    free(b);\n",
    "    free(out);\n",
    "    //Sugiero ver el comando: nvprof ./vector_add_parallel_multiblock\n",
    "}\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debe notarse que la traducción no es tan simple, por lo que se tiene que modificar el kernel para tener un comportamiento similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo: 555.781982421875 ms\n",
      "Error: 0.0\n"
     ]
    }
   ],
   "source": [
    "import pycuda.autoinit\n",
    "import pycuda.driver as drv\n",
    "import numpy\n",
    "\n",
    "from pycuda.compiler import SourceModule\n",
    "\n",
    "n = 400000000\n",
    "\n",
    "#Tomado de vector_add_parallel_multiblock\n",
    "mod = SourceModule(\"\"\"\n",
    "__global__ void vector_add(float *out, float *a, float *b, int n) {\n",
    "    int indice = blockIdx.x * blockDim.x + threadIdx.x;//Indice del thread que ejecuta el kernel\n",
    "    int paso = blockDim.x * gridDim.x;//El numero de threads por bloque\n",
    "    for(int i = indice; i < n; i+=paso){\n",
    "        out[i] = a[i] + b[i];\n",
    "    }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "vector_add = mod.get_function(\"vector_add\")\n",
    "\n",
    "n_streams = 50\n",
    "stream_size = int(n/n_streams)\n",
    "\n",
    "shape, dtype = (stream_size), numpy.float32\n",
    "\n",
    "a = [drv.pagelocked_empty(shape=shape, dtype=dtype) for i in range(n_streams)]\n",
    "b = [drv.pagelocked_empty(shape=shape, dtype=dtype) for i in range(n_streams)]\n",
    "dest = [drv.pagelocked_empty(shape=shape, dtype=dtype) for i in range(n_streams)]\n",
    "\n",
    "a_cuda = []\n",
    "b_cuda = []\n",
    "dest_cuda = []\n",
    "\n",
    "for i in range(n_streams):\n",
    "    a[i][:] = 1.0\n",
    "    b[i][:] = 2.0\n",
    "    a_cuda.append(drv.mem_alloc(a[0].nbytes))\n",
    "    b_cuda.append(drv.mem_alloc(b[0].nbytes))\n",
    "    dest_cuda.append(drv.mem_alloc(dest[0].nbytes))\n",
    "\n",
    "streams = [drv.Stream() for x in range(n_streams)]\n",
    "start = drv.Event()\n",
    "stop = drv.Event()\n",
    "start.record()\n",
    "for i, stream in enumerate(streams):\n",
    "    offset = i * stream_size\n",
    "    \n",
    "    drv.memcpy_htod_async(a_cuda[i], a[i], stream)\n",
    "    drv.memcpy_htod_async(b_cuda[i], b[i], stream)\n",
    "    \n",
    "    vector_add(\n",
    "        dest_cuda[i],\n",
    "        a_cuda[i],\n",
    "        b_cuda[i],\n",
    "        numpy.int32(stream_size),\n",
    "        block=(256,1,1),\n",
    "        grid=(100,1),\n",
    "        stream = stream)\n",
    "    \n",
    "    drv.memcpy_dtoh_async(dest[i], dest_cuda[i], stream)\n",
    "    \n",
    "drv.Context.synchronize()\n",
    "stop.record()\n",
    "stop.synchronize()\n",
    "time = start.time_till(stop)\n",
    "print(\"Tiempo: \"+str(time) + \" ms\")\n",
    "\n",
    "error = 0\n",
    "for i in range(n_streams):\n",
    "    error += dest[i].sum() - 3.0*len(dest[i])\n",
    "\n",
    "print(\"Error: \"+str(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver, es menos transparente que incluso CUDA-C debido a la dificultad de usar direcciones de memoria directas. Sin embargo el tiempo de ejecución es idéntico al de CUDA-C (dentro del GPU, en Python claramente no)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uso de GPUArray\n",
    "Una abstracción de PyCUDA es el GPUArray, que permite generar arreglos que vuelvan las operaciones vectorizadas transparentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "cuMemAlloc failed: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-00e05b16a3ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m400000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0ma_gpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpuarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mb_gpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpuarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/almac/mario_aor/miniconda3/envs/my_env/lib/python3.8/site-packages/pycuda/gpuarray.py\u001b[0m in \u001b[0;36mto_gpu\u001b[0;34m(ary, allocator)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mto_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallocator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmem_alloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m     \u001b[0;34m\"\"\"converts a numpy array to a GPUArray\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPUArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallocator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_compact_strides\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/almac/mario_aor/miniconda3/envs/my_env/lib/python3.8/site-packages/pycuda/gpuarray.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, shape, dtype, allocator, base, gpudata, strides, order)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgpudata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpudata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallocator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpudata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: cuMemAlloc failed: out of memory"
     ]
    }
   ],
   "source": [
    "import pycuda.gpuarray as gpuarray\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "import numpy\n",
    "\n",
    "a = numpy.ones(100000000).astype(numpy.float32)*2\n",
    "b = numpy.ones(100000000).astype(numpy.float32)\n",
    "\n",
    "a_gpu = gpuarray.to_gpu(a)\n",
    "b_gpu = gpuarray.to_gpu(b)\n",
    "\n",
    "a_resultado = (a_gpu + b_gpu).get()\n",
    "error = a_resultado.sum() - 100000000 * 3.0\n",
    "print(\"Error: \"+str(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, no es tan transparente y sencillo como parece."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
