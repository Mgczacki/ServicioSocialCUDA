{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA toolkit 10.2.89, artifact installation\n",
      "CUDA driver 10.2.0\n",
      "NVIDIA driver 430.26.0\n",
      "\n",
      "Libraries: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Your Tesla K20m GPU does not meet the minimal required compute capability (3.5.0 < 5.0).\n",
      "│ Some functionality might not work. For a fully-supported set-up, please use an older version of CUDA.jl\n",
      "└ @ CUDA /homen1/mario_aor/.julia/packages/CUDA/wTQsK/src/state.jl:251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- CUBLAS: 10.2.2\n",
      "- CURAND: 10.1.2\n",
      "- CUFFT: 10.1.2\n",
      "- CUSOLVER: 10.3.0\n",
      "- CUSPARSE: 10.3.1\n",
      "- CUPTI: 12.0.0\n",
      "- NVML: 10.0.0+430.26\n",
      "- CUDNN: 8.0.4 (for CUDA 10.2.0)\n",
      "- CUTENSOR: 1.2.1 (for CUDA 10.2.0)\n",
      "\n",
      "Toolchain:\n",
      "- Julia: 1.5.3\n",
      "- LLVM: 9.0.1\n",
      "- PTX ISA support: 3.2, 4.0, 4.1, 4.2, 4.3, 5.0, 6.0, 6.1, 6.3, 6.4\n",
      "- Device support: sm_30, sm_32, sm_35, sm_37, sm_50, sm_52, sm_53, sm_60, sm_61, sm_62, sm_70, sm_72, sm_75\n",
      "\n",
      "1 device:\n",
      "  0: Tesla K20m (sm_35, 4.534 GiB / 4.633 GiB available)\n"
     ]
    }
   ],
   "source": [
    "CUDA.versioninfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Array{VersionNumber,1}:\n",
       " v\"3.5.0\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[CUDA.capability(dev) for dev in CUDA.devices()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@assert CUDA.functional(true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA.jl propone el uso de tres capas de abstracción:\n",
    " - CuArray, que es un tipo de arreglos que implícitamente utiliza el GPU.\n",
    " - Programación de kernels con @cuda, en caso de tener cuellos de botella con los CuArrays\n",
    " - CUDA API wrappers, que permiten la utilización de características avanzadas de CUDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024-element CuArray{Float32,1}:\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " ⋮\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = CUDA.ones(1024) * 5\n",
    "b = CUDA.ones(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024-element CuArray{Float32,1}:\n",
       " 6.0\n",
       " 6.0\n",
       " 6.0\n",
       " 6.0\n",
       " 6.0\n",
       " 6.0\n",
       " 6.0\n",
       " 6.0\n",
       " 6.0\n",
       " 6.0\n",
       " 6.0\n",
       " 6.0\n",
       " 6.0\n",
       " ⋮\n",
       " 6.0\n",
       " 6.0\n",
       " 6.0\n",
       " 6.0\n",
       " 6.0\n",
       " 6.0\n",
       " 6.0\n",
       " 6.0\n",
       " 6.0\n",
       " 6.0\n",
       " 6.0\n",
       " 6.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024-element CuArray{Float32,1}:\n",
       " 9.0\n",
       " 9.0\n",
       " 9.0\n",
       " 9.0\n",
       " 9.0\n",
       " 9.0\n",
       " 9.0\n",
       " 9.0\n",
       " 9.0\n",
       " 9.0\n",
       " 9.0\n",
       " 9.0\n",
       " 9.0\n",
       " ⋮\n",
       " 9.0\n",
       " 9.0\n",
       " 9.0\n",
       " 9.0\n",
       " 9.0\n",
       " 9.0\n",
       " 9.0\n",
       " 9.0\n",
       " 9.0\n",
       " 9.0\n",
       " 9.0\n",
       " 9.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c .+ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024-element CuArray{Float32,1}:\n",
       " 6.0\n",
       " 6.0\n",
       " 6.0\n",
       " 6.0\n",
       " 6.0\n",
       " 6.0\n",
       " 6.0\n",
       " 6.0\n",
       " 6.0\n",
       " 6.0\n",
       " 6.0\n",
       " 6.0\n",
       " 6.0\n",
       " ⋮\n",
       " 6.0\n",
       " 6.0\n",
       " 6.0\n",
       " 6.0\n",
       " 6.0\n",
       " 6.0\n",
       " 6.0\n",
       " 6.0\n",
       " 6.0\n",
       " 6.0\n",
       " 6.0\n",
       " 6.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024-element Array{Int64,1}:\n",
       "    1\n",
       "    2\n",
       "    3\n",
       "    4\n",
       "    5\n",
       "    6\n",
       "    7\n",
       "    8\n",
       "    9\n",
       "   10\n",
       "   11\n",
       "   12\n",
       "   13\n",
       "    ⋮\n",
       " 1013\n",
       " 1014\n",
       " 1015\n",
       " 1016\n",
       " 1017\n",
       " 1018\n",
       " 1019\n",
       " 1020\n",
       " 1021\n",
       " 1022\n",
       " 1023\n",
       " 1024"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr1 = collect(1:1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "GPU compilation of kernel broadcast_kernel(CUDA.CuKernelContext, CuDeviceArray{Float32,1,1}, Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64}},typeof(+),Tuple{Base.Broadcast.Extruded{CuDeviceArray{Float32,1,1},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Extruded{Array{Int64,1},Tuple{Bool},Tuple{Int64}}}}, Int64) failed\nKernelError: passing and using non-bitstype argument\n\nArgument 4 to your kernel function is of type Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64}},typeof(+),Tuple{Base.Broadcast.Extruded{CuDeviceArray{Float32,1,1},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Extruded{Array{Int64,1},Tuple{Bool},Tuple{Int64}}}}, which is not isbits:\n  .args is of type Tuple{Base.Broadcast.Extruded{CuDeviceArray{Float32,1,1},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Extruded{Array{Int64,1},Tuple{Bool},Tuple{Int64}}} which is not isbits.\n    .2 is of type Base.Broadcast.Extruded{Array{Int64,1},Tuple{Bool},Tuple{Int64}} which is not isbits.\n      .x is of type Array{Int64,1} which is not isbits.\n\n",
     "output_type": "error",
     "traceback": [
      "GPU compilation of kernel broadcast_kernel(CUDA.CuKernelContext, CuDeviceArray{Float32,1,1}, Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64}},typeof(+),Tuple{Base.Broadcast.Extruded{CuDeviceArray{Float32,1,1},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Extruded{Array{Int64,1},Tuple{Bool},Tuple{Int64}}}}, Int64) failed\nKernelError: passing and using non-bitstype argument\n\nArgument 4 to your kernel function is of type Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64}},typeof(+),Tuple{Base.Broadcast.Extruded{CuDeviceArray{Float32,1,1},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Extruded{Array{Int64,1},Tuple{Bool},Tuple{Int64}}}}, which is not isbits:\n  .args is of type Tuple{Base.Broadcast.Extruded{CuDeviceArray{Float32,1,1},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Extruded{Array{Int64,1},Tuple{Bool},Tuple{Int64}}} which is not isbits.\n    .2 is of type Base.Broadcast.Extruded{Array{Int64,1},Tuple{Bool},Tuple{Int64}} which is not isbits.\n      .x is of type Array{Int64,1} which is not isbits.\n\n",
      "",
      "Stacktrace:",
      " [1] check_invocation(::GPUCompiler.CompilerJob, ::LLVM.Function) at /homen1/mario_aor/.julia/packages/GPUCompiler/uTpNx/src/validation.jl:68",
      " [2] macro expansion at /homen1/mario_aor/.julia/packages/GPUCompiler/uTpNx/src/driver.jl:238 [inlined]",
      " [3] macro expansion at /homen1/mario_aor/.julia/packages/TimerOutputs/ZmKD7/src/TimerOutput.jl:206 [inlined]",
      " [4] codegen(::Symbol, ::GPUCompiler.CompilerJob; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, validate::Bool, only_entry::Bool) at /homen1/mario_aor/.julia/packages/GPUCompiler/uTpNx/src/driver.jl:237",
      " [5] compile(::Symbol, ::GPUCompiler.CompilerJob; libraries::Bool, deferred_codegen::Bool, optimize::Bool, strip::Bool, validate::Bool, only_entry::Bool) at /homen1/mario_aor/.julia/packages/GPUCompiler/uTpNx/src/driver.jl:39",
      " [6] compile at /homen1/mario_aor/.julia/packages/GPUCompiler/uTpNx/src/driver.jl:35 [inlined]",
      " [7] cufunction_compile(::GPUCompiler.FunctionSpec; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /homen1/mario_aor/.julia/packages/CUDA/wTQsK/src/compiler/execution.jl:302",
      " [8] cufunction_compile(::GPUCompiler.FunctionSpec) at /homen1/mario_aor/.julia/packages/CUDA/wTQsK/src/compiler/execution.jl:297",
      " [9] check_cache(::Dict{UInt64,Any}, ::Any, ::Any, ::GPUCompiler.FunctionSpec{GPUArrays.var\"#broadcast_kernel#12\",Tuple{CUDA.CuKernelContext,CuDeviceArray{Float32,1,1},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64}},typeof(+),Tuple{Base.Broadcast.Extruded{CuDeviceArray{Float32,1,1},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Extruded{Array{Int64,1},Tuple{Bool},Tuple{Int64}}}},Int64}}, ::UInt64; kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /homen1/mario_aor/.julia/packages/GPUCompiler/uTpNx/src/cache.jl:40",
      " [10] broadcast_kernel at /homen1/mario_aor/.julia/packages/GPUArrays/WV76E/src/host/broadcast.jl:60 [inlined]",
      " [11] cached_compilation at /homen1/mario_aor/.julia/packages/GPUCompiler/uTpNx/src/cache.jl:65 [inlined]",
      " [12] cufunction(::GPUArrays.var\"#broadcast_kernel#12\", ::Type{Tuple{CUDA.CuKernelContext,CuDeviceArray{Float32,1,1},Base.Broadcast.Broadcasted{Nothing,Tuple{Base.OneTo{Int64}},typeof(+),Tuple{Base.Broadcast.Extruded{CuDeviceArray{Float32,1,1},Tuple{Bool},Tuple{Int64}},Base.Broadcast.Extruded{Array{Int64,1},Tuple{Bool},Tuple{Int64}}}},Int64}}; name::Nothing, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /homen1/mario_aor/.julia/packages/CUDA/wTQsK/src/compiler/execution.jl:289",
      " [13] cufunction at /homen1/mario_aor/.julia/packages/CUDA/wTQsK/src/compiler/execution.jl:286 [inlined]",
      " [14] macro expansion at /homen1/mario_aor/.julia/packages/CUDA/wTQsK/src/compiler/execution.jl:100 [inlined]",
      " [15] #launch_heuristic#857 at /homen1/mario_aor/.julia/packages/CUDA/wTQsK/src/gpuarrays.jl:17 [inlined]",
      " [16] launch_heuristic at /homen1/mario_aor/.julia/packages/CUDA/wTQsK/src/gpuarrays.jl:17 [inlined]",
      " [17] copyto! at /homen1/mario_aor/.julia/packages/GPUArrays/WV76E/src/host/broadcast.jl:66 [inlined]",
      " [18] copyto! at ./broadcast.jl:886 [inlined]",
      " [19] copy at ./broadcast.jl:862 [inlined]",
      " [20] materialize(::Base.Broadcast.Broadcasted{CUDA.CuArrayStyle{1},Nothing,typeof(+),Tuple{CuArray{Float32,1},Array{Int64,1}}}) at ./broadcast.jl:837",
      " [21] top-level scope at In[10]:1",
      " [22] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"
     ]
    }
   ],
   "source": [
    "c .+ arr1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024-element CuArray{Int64,1}:\n",
       "    1\n",
       "    2\n",
       "    3\n",
       "    4\n",
       "    5\n",
       "    6\n",
       "    7\n",
       "    8\n",
       "    9\n",
       "   10\n",
       "   11\n",
       "   12\n",
       "   13\n",
       "    ⋮\n",
       " 1013\n",
       " 1014\n",
       " 1015\n",
       " 1016\n",
       " 1017\n",
       " 1018\n",
       " 1019\n",
       " 1020\n",
       " 1021\n",
       " 1022\n",
       " 1023\n",
       " 1024"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpuarr1 = CUDA.CuArray(arr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024-element CuArray{Float32,1}:\n",
       "    1.0\n",
       "    2.0\n",
       "    3.0\n",
       "    4.0\n",
       "    5.0\n",
       "    6.0\n",
       "    7.0\n",
       "    8.0\n",
       "    9.0\n",
       "   10.0\n",
       "   11.0\n",
       "   12.0\n",
       "   13.0\n",
       "    ⋮\n",
       " 1013.0\n",
       " 1014.0\n",
       " 1015.0\n",
       " 1016.0\n",
       " 1017.0\n",
       " 1018.0\n",
       " 1019.0\n",
       " 1020.0\n",
       " 1021.0\n",
       " 1022.0\n",
       " 1023.0\n",
       " 1024.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpuarr1 = CUDA.CuArray{Float32}(arr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024-element CuArray{Float32,1}:\n",
       "    7.0\n",
       "    8.0\n",
       "    9.0\n",
       "   10.0\n",
       "   11.0\n",
       "   12.0\n",
       "   13.0\n",
       "   14.0\n",
       "   15.0\n",
       "   16.0\n",
       "   17.0\n",
       "   18.0\n",
       "   19.0\n",
       "    ⋮\n",
       " 1019.0\n",
       " 1020.0\n",
       " 1021.0\n",
       " 1022.0\n",
       " 1023.0\n",
       " 1024.0\n",
       " 1025.0\n",
       " 1026.0\n",
       " 1027.0\n",
       " 1028.0\n",
       " 1029.0\n",
       " 1030.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpuarr2 = c + gpuarr1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024-element CuArray{Float32,1}:\n",
       " 0.0\n",
       " 0.6931472\n",
       " 1.0986123\n",
       " 1.3862944\n",
       " 1.609438\n",
       " 1.7917595\n",
       " 1.9459102\n",
       " 2.0794415\n",
       " 2.1972246\n",
       " 2.3025851\n",
       " 2.3978953\n",
       " 2.4849067\n",
       " 2.5649493\n",
       " ⋮\n",
       " 6.9206715\n",
       " 6.921658\n",
       " 6.922644\n",
       " 6.923629\n",
       " 6.9246125\n",
       " 6.9255953\n",
       " 6.926577\n",
       " 6.927558\n",
       " 6.928538\n",
       " 6.929517\n",
       " 6.930495\n",
       " 6.931472"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.(gpuarr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024-element CuArray{Float32,1}:\n",
       " 0.0\n",
       " 0.6931472\n",
       " 1.0986123\n",
       " 1.3862944\n",
       " 1.609438\n",
       " 1.7917595\n",
       " 1.9459102\n",
       " 2.0794415\n",
       " 2.1972246\n",
       " 2.3025851\n",
       " 2.3978953\n",
       " 2.4849067\n",
       " 2.5649493\n",
       " ⋮\n",
       " 6.9206715\n",
       " 6.921658\n",
       " 6.922644\n",
       " 6.923629\n",
       " 6.9246125\n",
       " 6.9255953\n",
       " 6.926577\n",
       " 6.927558\n",
       " 6.928538\n",
       " 6.929517\n",
       " 6.930495\n",
       " 6.931472"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(log, gpuarr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective GPU memory usage: 3.15% (149.250 MiB/4.633 GiB)\n",
      "CUDA allocator usage: 48.043 MiB\n",
      "binned usage: 48.043 MiB (32.043 MiB allocated, 16.000 MiB cached)\n"
     ]
    }
   ],
   "source": [
    "CUDA.memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = CuArray{Int}(undef, 2000000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective GPU memory usage: 3.15% (149.250 MiB/4.633 GiB)\n",
      "CUDA allocator usage: 48.043 MiB\n",
      "binned usage: 48.043 MiB (48.043 MiB allocated, 0 bytes cached)\n"
     ]
    }
   ],
   "source": [
    "CUDA.memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective GPU memory usage: 3.15% (149.250 MiB/4.633 GiB)\n",
      "CUDA allocator usage: 48.043 MiB\n",
      "binned usage: 48.043 MiB (48.043 MiB allocated, 0 bytes cached)\n"
     ]
    }
   ],
   "source": [
    "CUDA.memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = CuArray{Int}(undef, 2000000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA.unsafe_free!(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective GPU memory usage: 2.13% (101.250 MiB/4.633 GiB)\n",
      "CUDA allocator usage: 44.000 KiB\n",
      "binned usage: 44.000 KiB (44.000 KiB allocated, 0 bytes cached)\n"
     ]
    }
   ],
   "source": [
    "CUDA.memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
